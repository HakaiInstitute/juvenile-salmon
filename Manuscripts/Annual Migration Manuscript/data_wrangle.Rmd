---
title: "data wrangle"
author: "Brett Johnson"
date: "October 16, 2018"
output: html_document
---

```{r setup}
library(hakaisalmon) v0.2.0
library(tidyverse)
library(lubridate)
library(knitr)
library(here)
library(car)

survey_seines <- hakaisalmon::survey_seines %>% 
  mutate(year = year(survey_date)) %>% 
  filter(sampling_week != "July 13")

saveRDS(survey_seines, here::here("data", "survey_seines.RDS"))
write_csv(survey_seines, here::here("data", "survey_seines.csv"))

```

```{r sealice}
sealice_lab_motiles <- hakaisalmon::sealice_lab_motiles
sealice_field <- hakaisalmon::sealice_field

lab_lice <- left_join(survey_seines, fish_field_data) %>%
  left_join(sealice_lab_motiles) %>%
  drop_na(cm_lab) %>%
  mutate(
    motile_caligus_lab = rowSums(
      select_(., "cm_lab", "cpaf_lab", "caf_lab", "cgf_lab", "ucal_lab"),
      na.rm = T
    ),
    motile_lep_lab = rowSums(
      select_(
        .,
        "lpam_lab",
        "lpaf_lab",
        "lam_lab",
        "laf_lab",
        "lgf_lab",
        "ulep_lab"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    semsp_id,
    species,
    region,
    survey_date,
    motile_caligus_lab,
    motile_lep_lab
  )

field_lice <- left_join(survey_seines, fish_field_data) %>%
  left_join(sealice_field) %>%
  # here I drop NAs from cm_field so that they wont be converted to zeros when rowSums na.rm = T coerces them to zero
  # the NAs will be created when I subsequently left_join
  drop_na(cm_field) %>%
  mutate(
    motile_caligus_field = rowSums(select_(
      ., "cm_field", "cpaf_field", "caf_field", "cgf_field"
    )),
    motile_lep_field = rowSums(
      select(
        .,
        "lpam_field",
        "lpaf_field",
        "lam_field",
        "laf_field",
        "lgf_field"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    semsp_id,
    species,
    region,
    survey_date,
    motile_caligus_field,
    motile_lep_field
  )

motile_lice <- full_join(lab_lice, field_lice)

sealice_time_series <- motile_lice %>%
  # with preference to lab ID, combine field and lab ID columns into one column
  mutate(motile_caligus = coalesce(motile_caligus_lab, motile_caligus_field)) %>%
  mutate(motile_lep = coalesce(motile_lep_lab, motile_lep_field)) %>%
  select(ufn,
         survey_date,
         site_id,
         region,
         species,
         motile_lep,
         motile_caligus) %>%
  mutate(sampling_week = as.numeric((yday(survey_date) + 4) %/% 7)) %>%
  filter(sampling_week <= 28, species %in% c("SO", "PI", "CU")) %>%
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  gather(`motile_caligus`,
         `motile_lep`,
         key = louse_species,
         value = n_lice) %>%
  drop_na() %>%
  mutate(year = year(survey_date),
         sampling_week = as.numeric((yday(survey_date) + 4) %/% 7))

sealice_time_series <- sealice_time_series %>%
  mutate(
    sampling_week = recode_factor(
      sealice_time_series$sampling_week,
      `18` = "May 5",
      `19` = "May 12" ,
      `20` = "May 19",
      `21` = "May 26",
      `22` = "June 2",
      `23` = "June 9",
      `24` = "June 16",
      `25` = "June 23",
      `26` = "June 30",
      `27` = "July 6",
      `28` = "July 13"
    )
  )

## Generate sealice prevalence column
motile_infected_hosts <- sealice_time_series %>%
  filter(n_lice > 0, species %in% c("SO", "PI", "CU")) %>%
  group_by(year, region, species, sampling_week, louse_species) %>%
  summarise(n_infected = n())

hosts <- sealice_time_series %>%
  group_by(year, region, species, sampling_week, louse_species) %>%
  summarise(n_examined = n()) %>%
  filter(species %in% c("SO", "PI", "CU"))

summary_sealice <- left_join(hosts, motile_infected_hosts) %>%
  replace_na(list(n_infected = 0)) %>%
  mutate(prevalence = n_infected /  n_examined) %>%
  select(-n_infected, -n_examined)

## Generate abundance column for summary_sealice
abundance <- sealice_time_series %>%
  select(year, region, species, sampling_week, louse_species, n_lice) %>%
  group_by(year, region, species, sampling_week, louse_species) %>%
  summarise(abundance = mean(n_lice, na.rm = T))

summary_sealice <- left_join(summary_sealice, abundance)

## Generate intensity column
intensity <- sealice_time_series %>%
  filter(n_lice > 0) %>%
  select(year, region, species, sampling_week, louse_species, n_lice) %>%
  group_by(year, region, species, sampling_week, louse_species) %>%
  summarise(intensity =  mean(n_lice, na.rm = T))

summary_sealice <- left_join(summary_sealice, intensity)

saveRDS(summary_sealice, here::here("data", "summary_sealice.RDS"))
write_csv(summary_sealice, here::here("data", "summary_sealice.csv"))
```

```{r SST}
## Get CTD data from EIMS database using R API

# Run this line indpendently,check console for URL, and authorize
client <- hakaiApi::Client$new()

qu39_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=QU39&limit=-1")

qu39_all <- client$get(qu39_endpoint) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

qu29_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=QU29&limit=-1")
qu29_all <- client$get(qu29_endpoint) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js2_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=JS2&limit=-1")
js2_all <- client$get(js2_endpoint)  %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )


js12_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=JS12&limit=-1")
js12_all <- client$get(js12_endpoint)  %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js2_12_all <- rbind(js2_all, js12_all)

js2_12_all$station <- "js2_12"

## Create time series of average conditions which includes the current year, using a loess function

ctd_all <- rbind(qu39_all, qu29_all, js2_12_all) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt),
    week = week(start_dt)
  ) %>%
  filter(depth <= 30) %>%
  select(
    year,
    date,
    week,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, date, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  )

saveRDS(ctd_all, here::here("data", "ctd_all.RDS"))
write_csv(ctd_all, here::here("data", "ctd_all.csv"))

## Create current year data to compare to time series
ctd_post_time_series <- rbind(qu39_all, qu29_all, js2_12_all) %>%
  filter(year == 2018, yday > 32, yday < 213,  depth <= 30) %>%
  select(
    year,
    date,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  )


## SST ANOMALY DATA
## QU39
qu39_average <- ctd_all %>%
  filter(station == "QU39")

# Filter down to station of interest
qu39_this_year <- ctd_post_time_series %>%
  filter(station == "QU39")

temp.lo_qu39 <-
  loess(mean_temp ~ yday, qu39_average, SE = T, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu39 <-
  tibble(yday = seq(min(qu39_average$yday), max(qu39_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu39$predicted_mean_temp <-
  predict(temp.lo_qu39, sim_temp_data_qu39, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
qu39_temp_anomaly_data <-
  left_join(sim_temp_data_qu39, qu39_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>%
  add_row(
    station = "QU39",
    yday = 47.5,
    predicted_mean_temp = predict(temp.lo_qu39, 47.5),
    mean_temp = predict(temp.lo_qu39, 47.5)
  ) %>%
  add_row(
    station = "QU39",
    yday = 124,
    predicted_mean_temp = predict(temp.lo_qu39, 124),
    mean_temp = predict(temp.lo_qu39, 124)
  ) %>%
  add_row(
    station = "QU39",
    yday = (145 + 149) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (145 + 149) / 2),
    mean_temp = predict(temp.lo_qu39, (145 + 149) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (155 + 149) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (155 + 149) / 2),
    mean_temp = predict(temp.lo_qu39, (155 + 149) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (192 + 177) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (192 + 177) / 2),
    mean_temp = predict(temp.lo_qu39, (192 + 177) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = 211.5,
    predicted_mean_temp = predict(temp.lo_qu39, 211.5),
    mean_temp = predict(temp.lo_qu39, 211.5)
  )

# Create min and max for any given day of the time series
qu39_min_max <- qu39_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU39")

##QU29
qu29_average <- ctd_all %>%
  filter(station == "QU29")

# Filter down to station of interest
qu29_this_year <- ctd_post_time_series %>%
  filter(station == "QU29")

temp.lo_qu29 <- loess(mean_temp ~ yday, qu29_average, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu29 <-
  tibble(yday = seq(min(qu29_average$yday), max(qu29_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu29$predicted_mean_temp <-
  predict(temp.lo_qu29, sim_temp_data_qu29, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
qu29_temp_anomaly_data <-
  left_join(sim_temp_data_qu29, qu29_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>%
  add_row(
    station = "QU29",
    yday = (51 + 36) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (51 + 36) / 2),
    mean_temp = predict(temp.lo_qu29, (51 + 36) / 2)
  ) %>%
  add_row(
    station = "QU29",
    yday = (157 + 136) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (157 + 136) / 2),
    mean_temp = predict(temp.lo_qu29, (157 + 136) / 2)
  ) %>%
  add_row(
    station = "QU29",
    yday = (157 + 201) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (157 + 201) / 2),
    mean_temp = predict(temp.lo_qu29, (157 + 201) / 2)
  )
# Create min and max for any given day of the time series

qu29_min_max <- qu29_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU29")

## JS2+12
js2_12_average <- ctd_all %>%
  filter(station == "js2_12")

# Filter down to station of interest
js2_12_this_year <- ctd_post_time_series %>%
  filter(station == "js2_12")

temp.lo_js2_12 <-
  loess(mean_temp ~ yday,
        js2_12_average,
        SE = T,
        span = 0.65)

#create table for predicitions from loess function
sim_temp_data_js2_12 <-
  tibble(yday = seq(min(js2_12_average$yday), max(js2_12_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_js2_12$predicted_mean_temp <-
  predict(temp.lo_js2_12, sim_temp_data_js2_12, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
js2_12_temp_anomaly_data <-
  left_join(sim_temp_data_js2_12, js2_12_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff)

# Create min and max for any given day of the time series
js2_12_min_max <- js2_12_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "js2_12")


min_max_data <- rbind(js2_12_min_max, qu39_min_max, qu29_min_max)
saveRDS(min_max_data,  here::here("data", "min_max_temps.RDS"))
write_csv(min_max_data,  here::here("data", "min_max_temps.csv"))

average_temps <- rbind(qu39_average, qu29_average, js2_12_average)
saveRDS(average_temps, here::here("data", "average_temps.RDS"))
write_csv(average_temps, here::here("data", "average_temps.csv"))

temperature_anomaly_data <-
  rbind(js2_12_temp_anomaly_data,
        qu29_temp_anomaly_data,
        qu39_temp_anomaly_data)
saveRDS(temperature_anomaly_data,
        here::here("data", "temperature_anomaly_data.RDS"))
write_csv(temperature_anomaly_data,
        here::here("data", "temperature_anomaly_data.csv"))
```
```{r Migration Timing}
#CUMULATIVE ABUNDANCE

tidy_catch <- survey_seines %>%
  # remove ad-hoc collections from migration timing calcs
  filter(survey_type == "standard",
         collection_protocol == "SEMSP",
         set_type == "targeted") %>%
  # only include consistently sampled sites with similar catchabilities
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total,
    lat,
    lon,
    sampling_week
  ) %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

tidy_catch <- as.data.frame(tidy_catch)

tidy_catch <- tidy_catch %>%
  mutate(yday = yday(survey_date)) %>%
  # remove pinks from non pink years and constrain the sample to before July 9th
  filter(!(species == "pi_total" &
             year %in% c(2015, 2017)), yday < 190)

saveRDS(tidy_catch, here::here("data", "tidy_catch.RDS"))
write_csv(tidy_catch, here::here("data", 'tidy_catch.csv'))

# Create a table with time series average daily proportion for each species and each region for all years combined (2015-2018)
#Sockeye
so_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "so_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

# Create Discovery Islands time series of cumulative abundance migration timing
so_total_catch_expanded_cum_DI <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

so_total_predict_average_prop_DI <-
  log_cumul_abund(
    so_total_catch_expanded_cum_DI$percent,
    so_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "so_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create Johnstone Strait time series of cumulative abundance migration timing
so_total_catch_expanded_cum_JS <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "JS")

so_total_predict_average_prop_JS <-
  log_cumul_abund(
    so_total_catch_expanded_cum_JS$percent,
    so_total_catch_expanded_cum_JS$survey_date
  ) %>%
  mutate(region = "JS", species = "so_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

#Pink
pi_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "pi_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

# Create Discovery Islands time series of cumulative abundance migration timing
pi_total_catch_expanded_cum_DI <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

pi_total_predict_average_prop_DI <-
  log_cumul_abund(
    pi_total_catch_expanded_cum_DI$percent,
    pi_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "pi_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create Johnstone Strait time series of cumulative abundance migration timing
pi_total_catch_expanded_cum_JS <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "JS")

pi_total_predict_average_prop_JS <-
  log_cumul_abund(
    pi_total_catch_expanded_cum_JS$percent,
    pi_total_catch_expanded_cum_JS$survey_date
  ) %>%
  mutate(region = "JS", species = "pi_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Chum
cu_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "cu_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

# Create Discovery Islands time series of cumulative abundance migration timing
cu_total_catch_expanded_cum_DI <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

cu_total_predict_average_prop_DI <-
  log_cumul_abund(
    cu_total_catch_expanded_cum_DI$percent,
    cu_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "cu_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create Johnstone Strait time series of cumulative abundance migration timing
cu_total_catch_expanded_cum_JS <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "JS")

cu_total_predict_average_prop_JS <-
  log_cumul_abund(
    cu_total_catch_expanded_cum_JS$percent,
    cu_total_catch_expanded_cum_JS$survey_date
  ) %>%
  mutate(region = "JS", species = "cu_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create table of time series average migration timing for JS and DI separate
predict_average_prop <-
  rbind(
    pi_total_predict_average_prop_DI,
    pi_total_predict_average_prop_JS,
    so_total_predict_average_prop_DI,
    so_total_predict_average_prop_JS,
    cu_total_predict_average_prop_DI,
    cu_total_predict_average_prop_JS
  ) %>%
  drop_na(daily_percent)

saveRDS(predict_average_prop,
        here::here("data", "predict_average_prop.RDS"))
write_csv(predict_average_prop,
        here::here("data", "predict_average_prop.csv"))
```


```{r Species Proportions}
# SPECIES PROPORTIONS

spp_prop <- survey_seines %>%
  # remove ad-hoc collections from migration timing calcs
  filter(survey_type == "standard",
         collection_protocol == "SEMSP",
         set_type == "targeted") %>%
  # only include consistently sampled sites with similar catchabilities
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total,
    sampling_week
  ) %>%
  # Next I remove instances when no sockeye were caught. I'm doing this because in 2015
  # and 2016 we only enumerated catches in which we caught sockeye, whereas in
  # 2017 and 2018 we enumerated all seines. So to reduce the bias introduced
  # from the field method i filter the comparison down to what is comparable
  filter(so_total > 0) %>%
  # remove instances when not all species were enumerated by droping rows with NA
  drop_na() %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

spp_prop_expanded <-
  spp_prop[rep(row.names(spp_prop), spp_prop$n), 1:6] %>%
  mutate(yday = yday(survey_date), year = year(survey_date))

proportions <- spp_prop_expanded %>%
  group_by(year, species) %>%
  summarize(n = n()) %>%
  mutate(proportion = n / sum(n)) %>%
  ungroup()

saveRDS(proportions, here::here("data", "proportion.RDS"))
write_csv(proportions, here::here("data", "proportion.csv"))
```

```{r Heatmap}
# HEATMAP Create heatmaps for key parameters

#calculate migration date z-scores

tidy_catch <- survey_seines %>%
  filter(survey_type == "standard", collection_protocol == "SEMSP") %>%
  filter(set_type == "targeted") %>%
  # only include consistently sampled sites with similar catchabilities
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  )  %>%
  select(survey_date,
         seine_id,
         region,
         so_total,
         pi_total,
         cu_total,
         co_total,
         he_total) %>%
  # filter out instances when sockeye were not caught because in 2015 we only enumerated sets that had sockeye in them, and moving forward we plan on enumerating all sets. If we included sets that had zero sockeye in 2017 and 2018 then our CPUE of sockeye in 2015 and 2016 would be biased high. Therefore this abundance metrics are strictly based on seines which had sockeye and we are then measuring how many of each species are in a seine, from seines with sockeye, as our metric of all species abundance and proportion. This is the only way to resolved inconsistent sampling strategies between years.
  filter(so_total > 0) %>%
  mutate(year = year(survey_date)) %>%
  # remove instances when there was not a complete enumeration of all species in the seine
  drop_na() %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  )

peak_dates <-
  tidy_catch[rep(row.names(tidy_catch), tidy_catch$n), 1:6] %>%
  filter(species %in% c("so_total", "pi_total", "cu_total"),
         region == "DI") %>%
  mutate(yday = yday(survey_date)) %>%
  group_by(year, region, species) %>%
  summarise(n = n(), median = median(yday)) %>%
  mutate(species = replace(species, species == "so_total", "SO")) %>%
  mutate(species = replace(species, species == "pi_total", "PI")) %>%
  mutate(species = replace(species, species == "cu_total", "CU"))

saveRDS(peak_dates, here::here("data", "peak_dates.RDS"))
write_csv(peak_dates, here::here("data", "peak_dates.csv"))

# Length z-scores

fish_data <- left_join(fish_field_data, fish_lab_data) %>%
  # combine both fork length measurements
  mutate(
    fork_length_lab = as.numeric(fork_length),
    fork_length_field = as.numeric(fork_length_field),
    fork_length = coalesce(fork_length_lab, fork_length_field)
  )

length_histo <- left_join(survey_seines, fish_data)  %>%
  select(sampling_week, survey_date, region, species, fork_length) %>%
  drop_na(fork_length) %>%
  mutate(year = year(survey_date)) %>%
  drop_na(year) %>%
  drop_na(fork_length) %>%
  # Remove incidtentaly sampled species
  filter(species != "CK", species != "CO", species != "HE") %>%
  mutate(year = as.factor(year))

saveRDS(length_histo, here::here("data", "length_histo.RDS"))
write_csv(length_histo, here::here("data", "length_histo.csv"))
```